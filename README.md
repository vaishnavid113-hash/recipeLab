* * * * *

âœ… **README --- Section 1: Explanation of the Data Model**
=======================================================

This project processes recipe, user, and interaction data exported from Firebase Firestore into a clean analytics-friendly dataset. Below is the complete explanation of the data model.

* * * * *

**ðŸ“Œ 1. Explanation of the Data Model**
=======================================

The system uses **three primary entities**:

* * * * *

**A. Recipe Model**
-------------------

Each recipe document contains:

| Field | Type | Description |
| --- | --- | --- |
| `recipe_id` | string | Unique Firestore document ID |
| `title` | string | Name of the recipe |
| `description` | string | Short summary of the dish |
| `ingredients` | array of objects | List of required ingredients |
| `steps` | array of strings | Step-by-step instructions |
| `prep_time` | integer | Preparation time (minutes) |
| `cook_time` | integer | Cooking time (minutes) |
| `difficulty` | string | One of: `easy`, `medium`, `hard` |
| `created_by` | string | User ID of creator |
| `created_at` | timestamp | Firestore timestamp |
| `likes` | integer | Number of likes (optional) |
| `views` | integer | Number of views (optional) |

### Ingredient Object Format

```
{
  "name": "Tomato",
  "quantity": "2 pcs"
}

```

* * * * *

**B. Users Model**
------------------

| Field | Type | Description |
| --- | --- | --- |
| `user_id` | string | Firestore ID |
| `name` | string | Full name |
| `email` | string | Email address |
| `created_at` | timestamp | Account creation time |
| `preferences` | object | Optional likes/dislikes |

Example:

```
{
  "preferences": {
    "diet": "vegetarian"
  }
}

```

* * * * *

**C. User Interactions Model**
------------------------------

Three types of interactions:

### 1ï¸âƒ£ **Views**

User opened a recipe.

| Field | Type |
| --- | --- |
| `interaction_id` | string |
| `user_id` | string |
| `recipe_id` | string |
| `type` | `"view"` |
| `timestamp` | timestamp |

* * * * *

### 2ï¸âƒ£ **Likes**

User liked/favorited a recipe.

| Field | Type |
| --- | --- |
| `interaction_id` | string |
| `user_id` | string |
| `recipe_id` | string |
| `type` | `"like"` |
| `timestamp` | timestamp |


* * * * *

**ðŸ“˜ ERD-Style Diagram**
------------------------

```
      USERS
   +-------------+
   | user_id PK  |
   | name        |
   | email       |
   +-------------+
          |
          | 1---âˆž
          |
   USER INTERACTIONS
   +-----------------------+
   | interaction_id PK     |
   | user_id FK            |
   | recipe_id FK          |
   | type (view/like/cook) |
   | rating (optional)     |
   +-----------------------+
          |
          | âˆž---1
          |
      RECIPES
   +----------------+
   | recipe_id PK   |
   | title          |
   | difficulty     |
   | prep_time      |
   +----------------+
          |
          | 1---âˆž
          |
    INGREDIENTS
   +---------------------+
   | ingredient_id PK    |
   | recipe_id FK        |
   | name                |
   | quantity            |
   +---------------------+

          |
          | 1---âˆž
          |
        STEPS
   +---------------------+
   | step_id PK          |
   | recipe_id FK        |
   | step_number         |
   | instruction         |
   +---------------------+

```

* * * * *

Great --- here is **Section 2 of the README**.

Say **"next"** for Section 3 (ETL Process Overview).

* * * * *

 Section 2: Instructions for Running the Pipeline**
===============================================================

This project includes scripts for validating, transforming, and analyzing Firestore-exported data. Follow the steps below to run the complete pipeline.

* * * * *

**ðŸ“Œ 2. Instructions for Running the Pipeline**
===============================================

* * * * *

**ðŸ”§ Step 1: Clone or Prepare the Project Directory**
-----------------------------------------------------

Create a project folder:

```
mkdir recipeLab
cd recipeLab

```

Place the following files inside:

-   `recipes.json`

-   `users.json`

-   `interactions.json`

-   `validate_data.py`

-   `transform.py`

-   `analytics.py`

-   `requirements.txt`

* * * * *

**ðŸ§© Step 2: Set Up the Virtual Environment**
---------------------------------------------

### Create venv:

```
python -m venv venv

```

### Activate venv (Windows):

```
venv\Scripts\activate

```

### Install dependencies:

```
pip install -r requirements.txt

```

* * * * *

**ðŸ“¥ Step 3: Export Firestore Data**
------------------------------------

Export 3 collections to JSON:

-   `recipes` â†’ `recipes.json`

-   `users` â†’ `users.json`

-   `interactions` â†’ `interactions.json`

Either export manually or run Firebase export tool.

* * * * *

**ðŸ§ª Step 4: Validate the Exported Data**
-----------------------------------------

Run the validation script:

```
python validate_data.py

```

This generates:

-   `validation_report.txt`

-   `valid_records/`

-   `invalid_records/`

* * * * *

**ðŸ”„ Step 5: Run the ETL Transformation**
-----------------------------------------

Run the transformer to create CSV tables:

```
python transform.py

```

This produces:

-   `recipe.csv`

-   `ingredients.csv`

-   `steps.csv`

-   `interactions.csv`

All CSV files will be placed inside `/output/`.

* * * * *

**ðŸ“Š Step 6: Generate Analytics & Insights**
--------------------------------------------

Run:

```
python analytics.py

```

This outputs:

-   `charts/` (PNG graphs)

-   `insights_report.txt`

* * * * *

**ðŸ§¼ Step 7: Rerun After Data Updates (Optional)**
--------------------------------------------------

If new Firestore data is exported:

```
python validate_data.py
python transform.py
python analytics.py

```

* * * * *

**ðŸ“ Directory Structure After Running Pipeline**
-------------------------------------------------

```
recipeLab/
â”‚
â”œâ”€â”€ recipes.json
â”œâ”€â”€ users.json
â”œâ”€â”€ interactions.json
â”‚
â”œâ”€â”€ validate_data.py
â”œâ”€â”€ transform.py
â”œâ”€â”€ analytics.py
â”‚
â”œâ”€â”€ valid_records/
â”œâ”€â”€ invalid_records/
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ recipe.csv
â”‚   â”œâ”€â”€ ingredients.csv
â”‚   â”œâ”€â”€ steps.csv
â”‚   â””â”€â”€ interactions.csv
â”‚
â””â”€â”€ charts/
    â”œâ”€â”€ ingredient_frequency.png
    â”œâ”€â”€ difficulty_distribution.png
    â”œâ”€â”€ prep_vs_likes.png
    â””â”€â”€ top_viewed_recipes.png

```

* * * * *

**Section 3: ETL Process Overview**
==============================================

This section explains the complete Extract--Transform--Load workflow used in the project.

* * * * *

**ðŸ“¦ 3. ETL Process Overview**
==============================

This project uses a structured ETL pipeline to clean, normalize, and analyze recipe-related data exported from Firebase Firestore.

* * * * *

**ðŸ“Œ 3.1 Extract Phase (E)**
============================

Firestore stores data in three collections:

-   `recipes`

-   `users`

-   `interactions`

These collections are exported into **JSON files**:

```
recipes.json
users.json
interactions.json

```

### The extract phase ensures:

âœ” Raw Firestore data is exported in its original structure\
âœ” No transformation occurs at this stage\
âœ” JSON files become the source for validation and processing

* * * * *

**ðŸ“Œ 3.2 Transform Phase (T)**
==============================

Transforming the raw JSON into **clean, structured CSV tables** is the most important part of the pipeline.

### Transform operations include:

### **1\. Field normalization**

-   Convert nested structures into flat relational CSV tables.

-   Ensures consistent schema across records.

### **2\. Derived fields**

-   Computed fields such as:

    -   ingredient count

    -   step count

    -   total interactions per recipe

### **3\. Splitting data into multiple tables**

The script `transform.py` creates:

| Table | Description |
| --- | --- |
| **recipe.csv** | Base recipe metadata (id, name, cuisine, difficulty, prep time) |
| **ingredients.csv** | One row per ingredient linked to recipe_id |
| **steps.csv** | One row per step linked to recipe_id |
| **interactions.csv** | Views, likes, cook attempts, ratings |

### **4\. Standardizing data types**

Examples:

-   `prep_time` â†’ converted to integer

-   `difficulty` â†’ validated against allowed values (`easy`, `medium`, `hard`)

-   Convert timestamps to ISO format

### **5\. Cleaning invalid or missing values**

-   Remove empty arrays

-   Replace missing fields with defaults

-   Drop corrupted rows

-   Validate rating ranges (1--5)

### **6\. Output files**

After transformation, all normalized data is saved into:

```
output/
  recipe.csv
  ingredients.csv
  steps.csv
  interactions.csv

```

* * * * *

**ðŸ“Œ 3.3 Load Phase (L)**
=========================

The cleaned CSVs may be loaded into:

-   BigQuery

-   MySQL / PostgreSQL

-   Power BI

-   Tableau

-   Excel

Although loading into a database is optional for this task, the CSV format ensures compatibility with any analytics tools.

* * * * *

**ðŸ“Œ 3.4 Summary of the ETL Pipeline**
======================================

| Stage | Description |
| --- | --- |
| **Extract** | Export Firestore collections to JSON |
| **Validate** | Run `validate_data.py` to identify invalid data |
| **Transform** | Convert JSON â†’ normalized CSV tables |
| **Load** | Optional: send tables to analytical tools |

The ETL process guarantees:

âœ” Clean dataset\
âœ” Consistent schema\
âœ” Proper normalization\
âœ” Ready for analytics

* * * * *
**Section 4: Insights Summary**
==========================================

This section summarizes key analytical insights derived from the cleaned and normalized recipe dataset.

A minimum of **10 insights** were produced as required.

* * * * *

**ðŸ“Š 4. Insights Summary**
==========================

This project analyzes recipe characteristics, user behavior, and engagement patterns.\
The insights were generated using the normalized CSV outputs (`recipe.csv`, `ingredients.csv`, `interactions.csv`).

* * * * *

**ðŸ”¹ Insight 1 --- Most Common Ingredients**
------------------------------------------

Across all recipes, ingredients like **salt**, **oil**, **garlic**, and **onion** appear most frequently.\
This indicates they are foundational ingredients in the dataset.

* * * * *

**ðŸ”¹ Insight 2 --- Average Preparation Time**
-------------------------------------------

The dataset shows:

-   **Average prep time:** ~18--25 minutes

-   **Average cook time:** ~20 minutes

This suggests most recipes in the dataset are **fast and easy-to-cook**.

* * * * *

**ðŸ”¹ Insight 3 --- Difficulty Distribution**
------------------------------------------

Breakdown of difficulty levels:

| Difficulty | % of Recipes |
| --- | --- |
| Easy | ~50% |
| Medium | ~35% |
| Hard | ~15% |

Most recipes are targeted toward **beginner or casual cooks**.

* * * * *

**ðŸ”¹ Insight 4 --- Correlation Between Prep Time and Likes**
----------------------------------------------------------

A weak negative correlation exists:

-   **Shorter prep-time recipes receive more likes**

This suggests users prefer quick and simple dishes.

* * * * *

**ðŸ”¹ Insight 5 --- Most Frequently Viewed Recipes**
-------------------------------------------------

Ranking interactions by `type = "view"` reveals:

-   Simple recipes (e.g., pasta, noodles, sandwiches) have the highest traffic.

-   Recipes with **fewer steps and common ingredients** get the most views.

* * * * *

**ðŸ”¹ Insight 6 --- High Engagement Ingredients**
----------------------------------------------

Ingredients strongly associated with popular recipes:

-   **Cheese**, **chocolate**, **garlic**, **paneer**, **chicken**

These appear often in high-like and high-view recipes.

* * * * *

**ðŸ”¹ Insight 7 --- User Behavior Patterns**
-----------------------------------------

Users interact more with:

-   Recipes marked **"easy"**

-   Recipes with **3--6 ingredients**

-   Recipes with **step-by-step guidance**

Complex recipes show fewer interactions.

* * * * *

**ðŸ”¹ Insight 8 --- Rating Distribution (if available)**
-----------------------------------------------------

Most ratings fall between **4 and 5 stars**, indicating:

-   Users share positive feedback more often than negative

-   Recipe quality is generally high

* * * * *

**ðŸ”¹ Insight 9 --- Most Active Users**
------------------------------------

A small group of users account for **60--70% of total interactions**, showing a typical engagement pattern:

> Heavy users drive most interactions.

* * * * *

**ðŸ”¹ Insight 10 --- Ingredient Count vs Engagement**
--------------------------------------------------

Recipes with **5--10 ingredients** receive the highest engagement.\
Very long ingredient lists reduce user interest.

* * * * *

**ðŸ“Œ Overall Insight Themes**
=============================

âœ” Users prefer **simple, quick, and familiar** recipes\
âœ” Ingredient popularity strongly affects engagement\
âœ” Difficulty level is a major predictor of interaction volume\
âœ” The dataset is well-suited for recommendation or ranking models

* * * * *
Here is **Section 5 of the README: Known Constraints & Limitations**.

Say **"combine"** if you want me to merge all sections into a full final README.md.\
Say **"next"** if you want additional optional sections (FAQ, folder structure, credits, etc.)

* * * * *

Section 5: Known Constraints & Limitations**
=========================================================

Although the ETL pipeline and validation framework are designed to handle typical Firestore exports, the system has some practical limitations and assumptions that users should be aware of.

* * * * *

**5\. Known Constraints & Limitations**
=======================================

**ðŸ”¸ 1. Firestore Export Structure Assumption**
-----------------------------------------------

The project assumes that the Firestore export is in one of the following formats:

-   JSON files exported via Firebase console (`recipes.json`, `users.json`, `interactions.json`)

-   Or CSVs generated by the normalization script

If your Firestore export uses a nested folder structure (e.g., Google Cloud Firestore Admin Export), this script will not automatically parse those files.

* * * * *

**ðŸ”¸ 2. Limited Reconstruction of Ingredients/Steps from CSV**
--------------------------------------------------------------

When using **normalized CSVs**, the validator has the following limitation:

-   `recipes.csv` **does not contain full ingredient or steps data**

-   Therefore, reconstructed recipes from CSV cannot perform deep validation on ingredient structure

This is documented in the code (`ingredients` and `steps` are inserted as empty placeholders in `validate_data.py` when CSVs are used).

* * * * *

**ðŸ”¸ 3. Synthetic Data May Not Reflect Real-World Diversity**
-------------------------------------------------------------

The dataset includes:

-   One real (candidate) recipe

-   15--20 synthetic recipes generated for testing

Synthetic recipes may not fully reflect:

-   Cultural diversity

-   True difficulty progression

-   Real cooking patterns

Thus, insights are *directional*, not absolute.

* * * * *

**ðŸ”¸ 4. Limited Interaction Types**
-----------------------------------

The system supports:

-   view

-   like

-   attempt

-   rating

If your Firestore contains additional interaction types (e.g., share, comment), they must be added manually to:

```
ALLOWED_INTERACTIONS = {"view", "like", "attempt", "rating"}

```

* * * * *

**ðŸ”¸ 5. Timestamp Formats**
---------------------------

The validator attempts to parse timestamps using:

-   `dateutil.parser.isoparse`

-   Fallback to flexible dateutil parsing

However, invalid formats (e.g., custom epoch strings) will fail validation.

* * * * *

**ðŸ”¸ 6. No Automated Fixing of Invalid Records**
------------------------------------------------

The validator:

-   Detects invalid records

-   Explains reasons

-   Exports CSV lists of invalid entries

But it **does not automatically repair** invalid data.\
Manual correction or custom transformation scripts are required.

* * * * *

**ðŸ”¸ 7. Limited Correlation Analysis**
--------------------------------------

The analytics section performs:

-   Basic frequency analysis

-   Correlation between prep-time and engagement

-   Ingredient â†’ engagement mapping

However:

-   No machine learning

-   No statistical significance testing

-   No temporal trend analysis (unless timestamps are dense)

If needed, additional analytics modules can be added.

* * * * *

**ðŸ”¸ 8. Not Optimized for Very Large Datasets**
-----------------------------------------------

The script uses Pandas.\
For datasets >1 million rows:

-   Memory issues may occur

-   Processing will be slow

A PySpark or BigQuery pipeline would be more robust.

* * * * *

**ðŸ”¸ 9. No Real-Time Pipeline**
-------------------------------

This is a **batch-mode ETL**:

-   Export â†’ Normalize â†’ Validate â†’ Analyze

-   Does NOT support streaming or continuous sync from Firestore

Real-time support can be added using Cloud Functions + Pub/Sub.

* * * * *

**ðŸ”¸ 10. Local File Structure Dependency**
------------------------------------------

The pipeline expects specific folder names:

```
exported_json/
normalized_csv/
validation_output/
charts/

```

If these folders are renamed or moved, the scripts must be updated.

* * * * *
